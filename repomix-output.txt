This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-10T02:16:37.536Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
app.py
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: app.py
================
import streamlit as st
import pandas as pd
import os
from pathlib import Path
from collections import Counter
from nltk.util import ngrams
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import re
from datetime import datetime
import base64
import io

# Page config
st.set_page_config(
    page_title="SEO Content Analysis Tool",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Initialize session state
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'results' not in st.session_state:
    st.session_state.results = {}


def preprocess_text(text):
    """Clean and tokenize text for n-gram analysis."""
    if pd.isna(text) or not isinstance(text, str):
        return []

    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stop_words]
    return tokens


def extract_ngrams(tokens, n):
    """Extract n-grams from tokenized text."""
    n_grams = list(ngrams(tokens, n))
    return [' '.join(gram) for gram in n_grams]


def create_duplicate_rollup(df, content_type):
    """Create a comprehensive rollup of all duplicates across page types."""
    duplicated_mask = df.duplicated(subset=[content_type], keep=False)
    duplicates_df = df[duplicated_mask].copy()

    if duplicates_df.empty:
        return None, None

    rollup = duplicates_df.groupby(content_type).agg({
        'Full URL': list,
        'pagetype': list,
        'Meta Description' if content_type == 'Title' else 'Title': list
    }).reset_index()

    rollup['Duplicate_Count'] = rollup['Full URL'].apply(len)
    rollup['Unique_Pagetypes'] = rollup['pagetype'].apply(lambda x: len(set(x)))
    rollup['Pagetype_List'] = rollup['pagetype'].apply(lambda x: ', '.join(sorted(set(x))))
    rollup['URLs'] = rollup['Full URL'].apply(lambda x: '\n'.join(sorted(x)))

    if content_type == 'Title':
        rollup['Meta_Descriptions'] = rollup['Meta Description'].apply(
            lambda x: '\n'.join(sorted(set(str(desc) for desc in x)))
        )
        columns = [
            'Title', 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Meta_Descriptions', 'URLs'
        ]
    else:
        rollup['Titles'] = rollup['Title'].apply(
            lambda x: '\n'.join(sorted(set(str(title) for title in x)))
        )
        columns = [
            'Meta Description', 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Titles', 'URLs'
        ]

    rollup = rollup[columns].sort_values('Duplicate_Count', ascending=False)

    detailed_rows = []
    for _, row in duplicates_df.iterrows():
        detailed_rows.append({
            'Content': row[content_type],
            'URL': row['Full URL'],
            'Pagetype': row['pagetype'],
            'Other_Content': row['Meta Description'] if content_type == 'Title' else row['Title']
        })

    detailed_df = pd.DataFrame(detailed_rows)

    return rollup, detailed_df


def analyze_content(df, content_type, include_ngrams=True, ngram_sizes=[2, 3, 4]):
    """Analyze content (titles or meta descriptions) across page types."""
    rollup_df, detailed_df = create_duplicate_rollup(df, content_type)

    duplicates = df.groupby(content_type).agg({
        'Full URL': 'count',
        'pagetype': lambda x: ', '.join(set(x))
    }).reset_index()
    duplicates.columns = [content_type, 'Duplicate_Count', 'Pagetypes']
    duplicates = duplicates.sort_values('Duplicate_Count', ascending=False)

    pagetype_summary = df.groupby('pagetype').agg({
        'Full URL': 'count',
        content_type: lambda x: x.duplicated().sum()
    }).reset_index()
    pagetype_summary.columns = [
        'Pagetype', 'Total_URLs', f'Duplicate_{content_type}s'
    ]
    pagetype_summary['Duplication_Rate'] = (
        pagetype_summary[f'Duplicate_{content_type}s'] /
        pagetype_summary['Total_URLs'] * 100
    ).round(2)

    duplicate_analysis_by_pagetype = {}
    for pagetype in df['pagetype'].unique():
        pagetype_data = df[df['pagetype'] == pagetype]
        duplicates_in_pagetype = pagetype_data[
            pagetype_data.duplicated(subset=[content_type], keep=False)
        ]
        if not duplicates_in_pagetype.empty:
            duplicate_analysis_by_pagetype[pagetype] = duplicates_in_pagetype

    # N-gram analysis
    ngram_analyses = {}

    # Only perform n-gram analysis if requested
    if include_ngrams and len(ngram_sizes) > 0:
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
        except LookupError:
            with st.spinner('Downloading required NLTK data...'):
                nltk.download('punkt')
                nltk.download('stopwords')

        for n in ngram_sizes:
            ngram_counts_by_pagetype = {}

            for pagetype in df['pagetype'].unique():
                content = df[df['pagetype'] == pagetype][content_type]

                all_ngrams = []
                for text in content:
                    tokens = preprocess_text(text)
                    if tokens:
                        text_ngrams = extract_ngrams(tokens, n)
                        all_ngrams.extend(text_ngrams)

                ngram_counts = Counter(all_ngrams)
                frequent_ngrams = {
                    gram: count for gram, count in ngram_counts.items()
                    if count > 1
                }

                if frequent_ngrams:
                    ngram_counts_by_pagetype[pagetype] = frequent_ngrams

            rows = []
            for pagetype, ngram_counts in ngram_counts_by_pagetype.items():
                for ngram, count in ngram_counts.items():
                    rows.append({
                        'pagetype': pagetype,
                        'ngram': ngram,
                        'frequency': count
                    })

            if rows:
                ngram_df = pd.DataFrame(rows)
                ngram_df = ngram_df.sort_values(
                    ['pagetype', 'frequency'],
                    ascending=[True, False]
                )
                ngram_analyses[n] = ngram_df

    return (
        duplicates,
        pagetype_summary,
        duplicate_analysis_by_pagetype,
        ngram_analyses,
        rollup_df,
        detailed_df
    )


def get_csv_download_link(df, filename):
    """Generate a download link for a dataframe."""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = (
        f'<a href="data:file/csv;base64,{b64}" '
        f'download="{filename}">Download {filename}</a>'
    )
    return href

def main():
    st.title("SEO Content Analysis Tool")
    st.write("""
    This tool analyzes SEO content (titles and meta descriptions) across different page types,
    identifying patterns, duplicates, and n-gram frequencies.
    """)

    # File upload
    uploaded_file = st.file_uploader(
        "Upload your CSV/TSV file",
        type=['csv', 'tsv'],
        help="File should contain columns: Full URL, pagetype, Title, Meta Description"
    )

    if uploaded_file:
        # Configuration sidebar
        st.sidebar.header("Analysis Configuration")

        # File configuration
        st.sidebar.subheader("File Settings")
        delimiter = st.sidebar.selectbox(
            "File delimiter",
            options=[',', '\t', ';', '|'],
            index=0,
            format_func=lambda x: 'Tab' if x == '\t' else x
        )

        chunk_size = st.sidebar.number_input(
            "Chunk size",
            min_value=1000,
            max_value=1000000,
            value=100000,
            step=10000,
            help="Larger chunks process faster but use more memory"
        )

        # Analysis configuration
        st.sidebar.subheader("Analysis Options")
        analysis_type = st.sidebar.multiselect(
            "Select content to analyze",
            options=["Titles", "Meta Descriptions"],
            default=["Titles", "Meta Descriptions"],
            help="Choose which content types to analyze"
        )

        include_ngrams = st.sidebar.checkbox(
            "Include N-gram Analysis",
            value=True,
            help="Enable to analyze word patterns (may increase processing time)"
        )

        if include_ngrams:
            ngram_sizes = st.sidebar.multiselect(
                "Select N-gram sizes",
                options=["Bigrams (2)", "Trigrams (3)", "4-grams (4)"],
                default=["Bigrams (2)", "Trigrams (3)", "4-grams (4)"],
                help="Choose which n-gram sizes to analyze"
            )

        try:
            with st.spinner("Reading and processing file..."):
                df = pd.read_csv(uploaded_file, sep=delimiter)

                # Verify required columns
                required_columns = {'Full URL', 'pagetype', 'Title', 'Meta Description'}
                missing_columns = required_columns - set(df.columns)

                if missing_columns:
                    st.error(f"Missing required columns: {', '.join(missing_columns)}")
                    st.write("Available columns:", ', '.join(df.columns))
                    return

                # Display basic file info
                st.subheader("File Overview")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total URLs", f"{len(df):,}")
                with col2:
                    st.metric("Unique URLs", f"{df['Full URL'].nunique():,}")
                with col3:
                    st.metric("Page Types", f"{df['pagetype'].nunique():,}")

                # Analyze button
                if st.button("Run Analysis"):
                    with st.spinner("Analyzing content..."):
                        title_results = None
                        meta_results = None

                        # Title analysis
                        if "Titles" in analysis_type:
                            st.info("Analyzing titles...")
                            title_results = analyze_content(
                                df,
                                'Title',
                                include_ngrams=include_ngrams,
                                ngram_sizes=[int(s[0]) for s in ngram_sizes] if include_ngrams else []
                            )
                            (
                                title_duplicates, title_pagetype_summary,
                                title_duplicate_analysis, title_ngram_analyses,
                                title_rollup, title_detailed
                            ) = title_results

                        # Meta description analysis
                        if "Meta Descriptions" in analysis_type:
                            st.info("Analyzing meta descriptions...")
                            meta_results = analyze_content(
                                df,
                                'Meta Description',
                                include_ngrams=include_ngrams,
                                ngram_sizes=[int(s[0]) for s in ngram_sizes] if include_ngrams else []
                            )
                            (
                                meta_duplicates, meta_pagetype_summary,
                                meta_duplicate_analysis, meta_ngram_analyses,
                                meta_rollup, meta_detailed
                            ) = meta_results

                        # Store results in session state
                        st.session_state.results = {
                            'title': title_results,
                            'meta': meta_results
                        }
                        st.session_state.analysis_complete = True

                        st.success("Analysis complete!")
                        st.experimental_rerun()

                # Display results if analysis is complete
                if st.session_state.analysis_complete:
                    title_results = st.session_state.results['title']
                    meta_results = st.session_state.results['meta']

                    st.header("Analysis Results")

                    # Create dynamic tabs based on selected analyses
                    tab_options = []
                    if "Titles" in analysis_type:
                        tab_options.append("Title Analysis")
                    if "Meta Descriptions" in analysis_type:
                        tab_options.append("Meta Description Analysis")
                    if include_ngrams and len(ngram_sizes) > 0:
                        tab_options.append("N-gram Analysis")
                    tab_options.append("Export Results")

                    tabs = st.tabs(tab_options)

                    # Title Analysis Tab
                    with tabs[0]:
                        st.subheader("Title Duplicate Analysis")
                        if title_results[4] is not None:  # title_rollup
                            st.dataframe(title_results[4])
                            st.markdown(
                                get_csv_download_link(
                                    title_results[4],
                                    "title_duplicate_rollup.csv"
                                ),
                                unsafe_allow_html=True
                            )

                        st.subheader("Title Duplication by Page Type")
                        st.dataframe(title_results[1])  # title_pagetype_summary

                    # Meta Description Analysis Tab
                    if "Meta Descriptions" in analysis_type and meta_results is not None:
                        with tabs[1]:
                            st.subheader("Meta Description Duplicate Analysis")
                            if meta_results[4] is not None:  # meta_rollup
                                st.dataframe(meta_results[4])
                                st.markdown(
                                    get_csv_download_link(
                                        meta_results[4],
                                        "meta_description_duplicate_rollup.csv"
                                    ),
                                    unsafe_allow_html=True
                                )

                            st.subheader("Meta Description Duplication by Page Type")
                            st.dataframe(meta_results[1])  # meta_pagetype_summary

                    # N-gram Analysis Tab
                    if include_ngrams and len(ngram_sizes) > 0:
                        with tabs[2]:
                            if "Titles" in analysis_type and title_results is not None:
                                st.subheader("Title N-grams")
                                gram_tabs = st.tabs(["Bigrams", "Trigrams", "4-grams"])
                                for i, tab in enumerate(gram_tabs):
                                    with tab:
                                        n = i + 2
                                        if title_results[3] is not None and n in title_results[3]:
                                            st.dataframe(title_results[3][n])

                            if "Meta Descriptions" in analysis_type and meta_results is not None:
                                st.subheader("Meta Description N-grams")
                                gram_tabs = st.tabs(["Bigrams", "Trigrams", "4-grams"])

                        for i, tab in enumerate(gram_tabs):
                            with tab:
                                n = i + 2
                                if meta_results[3][n] is not None:  # ngram_analyses
                                    st.dataframe(meta_results[3][n])

                    # Export Results Tab
                    with tabs[3]:
                        st.subheader("Download Analysis Results")

                        # Create download buttons for all analysis files
                        if title_results[4] is not None:
                            st.markdown(
                                get_csv_download_link(
                                    title_results[4],
                                    "title_duplicate_rollup.csv"
                                ),
                                unsafe_allow_html=True
                            )

                        if meta_results[4] is not None:
                            st.markdown(
                                get_csv_download_link(
                                    meta_results[4],
                                    "meta_description_duplicate_rollup.csv"
                                ),
                                unsafe_allow_html=True
                            )

                        st.markdown(
                            get_csv_download_link(
                                title_results[1],
                                "title_pagetype_summary.csv"
                            ),
                            unsafe_allow_html=True
                        )

                        st.markdown(
                            get_csv_download_link(
                                meta_results[1],
                                "meta_description_pagetype_summary.csv"
                            ),
                            unsafe_allow_html=True
                        )

                        # N-gram downloads
                        st.subheader("N-gram Analysis Downloads")
                        for n in [2, 3, 4]:
                            if title_results[3][n] is not None:
                                st.markdown(
                                    get_csv_download_link(
                                        title_results[3][n],
                                        f"title_{n}gram_analysis.csv"
                                    ),
                                    unsafe_allow_html=True
                                )
                            if meta_results[3][n] is not None:
                                st.markdown(
                                    get_csv_download_link(
                                        meta_results[3][n],
                                        f"meta_description_{n}gram_analysis.csv"
                                    ),
                                    unsafe_allow_html=True
                                )

                        # Generate summary report
                        st.subheader("Summary Report")
                        summary_text = f"""SEO Content Analysis Summary Report
=================================

Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Overall Statistics
-----------------
Total URLs analyzed: {len(df):,}
Unique URLs: {df['Full URL'].nunique():,}
Total page types: {df['pagetype'].nunique():,}

Title Analysis
-------------
Total titles: {len(df):,}
Unique titles: {df['Title'].nunique():,}
Duplicate titles: {len(df[df.duplicated(subset=['Title'], keep=False)]):,}
"""
                        if title_results[4] is not None:
                            summary_text += (
                                f"Number of unique duplicate titles: {len(title_results[4]):,}\n"
                                f"Most repeated title: {title_results[4].iloc[0]['Duplicate_Count']:,} occurrences\n"
                            )

                        summary_text += f"""
Meta Description Analysis
------------------------
Total meta descriptions: {len(df):,}
Unique meta descriptions: {df['Meta Description'].nunique():,}
Duplicate meta descriptions: {len(df[df.duplicated(subset=['Meta Description'], keep=False)]):,}
"""
                        if meta_results[4] is not None:
                            summary_text += (
                                f"Number of unique duplicate meta descriptions: {len(meta_results[4]):,}\n"
                                f"Most repeated meta description: {meta_results[4].iloc[0]['Duplicate_Count']:,} occurrences\n"
                            )

                        summary_text += "\nAnalysis by Page Type\n--------------------\n"
                        for pagetype in sorted(df['pagetype'].unique()):
                            pagetype_data = df[df['pagetype'] == pagetype]
                            summary_text += f"\nPage Type: {pagetype}\n"
                            summary_text += f"Total URLs: {len(pagetype_data):,}\n"

                            title_dupes = len(pagetype_data[
                                pagetype_data.duplicated(subset=['Title'], keep=False)
                            ])
                            summary_text += f"Duplicate Titles: {title_dupes:,} "
                            if title_dupes > 0:
                                summary_text += f"({(title_dupes/len(pagetype_data)*100):.1f}%)\n"
                            else:
                                summary_text += "\n"

                            meta_dupes = len(pagetype_data[
                                pagetype_data.duplicated(subset=['Meta Description'], keep=False)
                            ])
                            summary_text += f"Duplicate Meta Descriptions: {meta_dupes:,} "
                            if meta_dupes > 0:
                                summary_text += f"({(meta_dupes/len(pagetype_data)*100):.1f}%)\n"
                            else:
                                summary_text += "\n"

                        # Display and provide download for summary report
                        st.text_area("Summary Report", summary_text, height=400)
                        b64 = base64.b64encode(summary_text.encode()).decode()
                        st.markdown(
                            f'<a href="data:text/plain;base64,{b64}" '
                            f'download="analysis_summary.txt">Download Summary Report</a>',
                            unsafe_allow_html=True
                        )

        except Exception as e:
            import traceback
            import sys
            
            # Get the full traceback
            exc_type, exc_value, exc_traceback = sys.exc_info()
            
            # Print to terminal/console
            print("\n=== Error Details ===")
            print("Exception type:", exc_type.__name__)
            print("Error message:", str(e))
            print("\nFull traceback:")
            traceback.print_tb(exc_traceback)
            print("==================\n")
            
            # Still show user-friendly message in UI
            st.error(f"Error processing file: {str(e)}")
            st.write("Please check your file format and delimiter settings.")
            
            # For debugging, you can also show the full traceback in the UI
            with st.expander("Show Debug Information"):
                st.code(traceback.format_exc())


if __name__ == "__main__":
    main()

================
File: README.md
================
# SEO Content Analysis Tool

## Overview
A Streamlit-based web application for analyzing SEO content across different page types. This tool helps identify patterns, duplicates, and n-gram frequencies in titles and meta descriptions, making it easier to maintain content consistency and identify optimization opportunities.

## Features
- Interactive web interface built with Streamlit
- Support for CSV and TSV file formats
- Comprehensive duplicate content analysis
- N-gram analysis (bigrams, trigrams, 4-grams)
- Page type breakdown and statistics
- Downloadable analysis reports
- Real-time progress tracking
- Interactive data visualization
- Configurable processing parameters

## Requirements
- Python 3.8+
- Required packages:
  ```
  streamlit>=1.28.0
  pandas>=1.5.0
  nltk>=3.8.1
  tqdm>=4.65.0
  ```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/seo-content-analysis.git
cd seo-content-analysis
```

2. Create and activate a virtual environment (recommended):
```bash
# On macOS/Linux
python -m venv venv
source venv/bin/activate

# On Windows
python -m venv venv
venv\Scripts\activate
```

3. Install required packages:
```bash
pip install -r requirements.txt
```

## Usage

1. Start the Streamlit app:
```bash
streamlit run app.py
```

2. Open your web browser and navigate to the provided URL (typically http://localhost:8501)

3. Upload your CSV/TSV file containing the following required columns:
   - Full URL
   - pagetype
   - Title
   - Meta Description

4. Configure analysis settings in the sidebar:
   - Select the appropriate file delimiter
   - Adjust chunk size if needed

5. Click "Run Analysis" to start processing

## Input File Requirements

Your input file should be a CSV or TSV with the following columns:
- `Full URL`: The complete URL of the page
- `pagetype`: The category or type of the page
- `Title`: The page's title tag content
- `Meta Description`: The page's meta description content

Example format:
```csv
Full URL,pagetype,Title,Meta Description
https://example.com/page1,blog,"First Blog Post","This is a description of the first blog post"
https://example.com/page2,product,"Product Name","Product description here"
```

## Analysis Features

### Title Analysis
- Identifies duplicate titles across all pages
- Groups duplicates by page type
- Calculates duplication rates
- Provides detailed duplicate instances

### Meta Description Analysis
- Finds duplicate meta descriptions
- Groups by page type
- Shows duplication patterns
- Lists affected URLs

### N-gram Analysis
- Analyzes frequent word patterns
- Supports 2-gram, 3-gram, and 4-gram analysis
- Breaks down patterns by page type
- Shows frequency counts

### Export Options
- CSV exports for all analyses
- Comprehensive summary report
- Detailed breakdown by page type
- N-gram frequency reports

## Output Files

The tool generates several downloadable files:
1. Duplicate Analysis:
   - `title_duplicate_rollup.csv`
   - `meta_description_duplicate_rollup.csv`
   - `title_pagetype_summary.csv`
   - `meta_description_pagetype_summary.csv`

2. N-gram Analysis:
   - `title_2gram_analysis.csv`
   - `title_3gram_analysis.csv`
   - `title_4gram_analysis.csv`
   - `meta_description_2gram_analysis.csv`
   - `meta_description_3gram_analysis.csv`
   - `meta_description_4gram_analysis.csv`

3. Summary:
   - `analysis_summary.txt`

## Performance Tips

- For large files (>1GB), increase the chunk size in the sidebar
- Close other memory-intensive applications when processing large datasets
- Use a machine with at least 8GB RAM for optimal performance

## Error Handling

The tool includes comprehensive error handling for:
- Missing or invalid columns
- File format issues
- Memory limitations
- Processing errors

If you encounter errors:
1. Check your input file format
2. Verify required columns are present
3. Adjust chunk size if needed
4. Ensure sufficient system resources

## Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

1. Fork the repository
2. Create your feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For bugs, questions, and discussions, please use the GitHub Issues page.

## Acknowledgments

- Built with Streamlit
- Uses NLTK for natural language processing
- Powered by pandas for data analysis

================
File: requirements.txt
================
streamlit>=1.28.0
pandas>=1.5.0
nltk>=3.8.1
tqdm>=4.65.0
