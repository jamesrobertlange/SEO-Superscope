This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-11T21:19:08.891Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.devcontainer/
  devcontainer.json
.gitattributes
.gitignore
app.py
notes.md
README.md
requirements.txt

================================================================
Repository Files
================================================================

================
File: .devcontainer/devcontainer.json
================
{
  "name": "Python 3",
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md",
        "app.py"
      ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo '✅ Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}

================
File: .gitattributes
================
# Auto detect text files and perform LF normalization
* text=auto

================
File: .gitignore
================
small-upload.csv

================
File: app.py
================
import streamlit as st
import pandas as pd
import os
from pathlib import Path
from collections import Counter
import re
from datetime import datetime
import base64
import io
from io import BytesIO
import traceback
import zipfile

# Page config
st.set_page_config(
    page_title="SEO Content Analysis Tool",
    page_icon="📊",
    layout="wide"
)

# Initialize ALL session state variables
if 'show_mapping' not in st.session_state:
    st.session_state.show_mapping = True
if 'show_analysis' not in st.session_state:
    st.session_state.show_analysis = False
if 'mapped_df' not in st.session_state:
    st.session_state.mapped_df = None
if 'mapping_complete' not in st.session_state:
    st.session_state.mapping_complete = False
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'results' not in st.session_state:
    st.session_state.results = {}

def preprocess_text(text):
    """Clean and tokenize text using regex instead of NLTK."""
    if pd.isna(text) or not isinstance(text, str):
        return []
    
    # Convert to lowercase and remove special characters
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    
    # Simple tokenization by splitting on whitespace
    tokens = text.split()
    
    # Basic English stop words list
    stop_words = {
        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 
        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', 
        'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',
        'have', 'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how',
        'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',
        'such', 'than', 'too', 'very', 'can', 'will', 'just'
    }
    
    # Remove stop words
    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]
    return tokens

def extract_ngrams(tokens, n):
    """Extract n-grams from tokenized text."""
    if len(tokens) < n:
        return []
    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

def create_duplicate_rollup(df, content_type):
    """Create a comprehensive rollup of all duplicates across page types."""
    duplicated_mask = df.duplicated(subset=[content_type], keep=False)
    duplicates_df = df[duplicated_mask].copy()

    if duplicates_df.empty:
        return None, None

    rollup = duplicates_df.groupby(content_type).agg({
        'Full URL': list,
        'pagetype': list,
        'Meta Description' if content_type == 'Title' else 'Title': list
    }).reset_index()

    rollup['Duplicate_Count'] = rollup['Full URL'].apply(len)
    rollup['Unique_Pagetypes'] = rollup['pagetype'].apply(lambda x: len(set(x)))
    rollup['Pagetype_List'] = rollup['pagetype'].apply(lambda x: ', '.join(sorted(set(x))))
    rollup['URLs'] = rollup['Full URL'].apply(lambda x: '\n'.join(sorted(x)))

    if content_type == 'Title':
        rollup['Meta_Descriptions'] = rollup['Meta Description'].apply(
            lambda x: '\n'.join(sorted(set(str(desc) for desc in x)))
        )
        columns = [
            'Title', 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Meta_Descriptions', 'URLs'
        ]
    else:
        rollup['Titles'] = rollup['Title'].apply(
            lambda x: '\n'.join(sorted(set(str(title) for title in x)))
        )
        columns = [
            'Meta Description', 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Titles', 'URLs'
        ]

    rollup = rollup[columns].sort_values('Duplicate_Count', ascending=False)

    detailed_rows = []
    for _, row in duplicates_df.iterrows():
        detailed_rows.append({
            'Content': row[content_type],
            'URL': row['Full URL'],
            'Pagetype': row['pagetype'],
            'Other_Content': row['Meta Description'] if content_type == 'Title' else row['Title']
        })

    detailed_df = pd.DataFrame(detailed_rows)

    return rollup, detailed_df

def analyze_content(df, content_type, include_ngrams=True, ngram_sizes=[2, 3, 4]):
    """Analyze content (titles or meta descriptions) across page types."""
    # Map the content type to the actual column name
    column_mapping = {
        'Title': 'title',
        'Meta Description': 'meta_description'
    }
    
    content_column = column_mapping.get(content_type, content_type)
    
    # Create duplicate rollup
    duplicated_mask = df.duplicated(subset=[content_column], keep=False)
    duplicates_df = df[duplicated_mask].copy()

    if duplicates_df.empty:
        return None, None, None, {}, None, None

    # Create rollup of duplicates
    rollup = duplicates_df.groupby(content_column).agg({
        'url': list,
        'pagetype': list,
        'meta_description' if content_column == 'title' else 'title': list
    }).reset_index()

    rollup['Duplicate_Count'] = rollup['url'].apply(len)
    rollup['Unique_Pagetypes'] = rollup['pagetype'].apply(lambda x: len(set(x)))
    rollup['Pagetype_List'] = rollup['pagetype'].apply(lambda x: ', '.join(sorted(set(x))))
    rollup['URLs'] = rollup['url'].apply(lambda x: '\n'.join(sorted(x)))

    if content_column == 'title':
        rollup['Meta_Descriptions'] = rollup['meta_description'].apply(
            lambda x: '\n'.join(sorted(set(str(desc) for desc in x)))
        )
        columns = [
            content_column, 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Meta_Descriptions', 'URLs'
        ]
    else:
        rollup['Titles'] = rollup['title'].apply(
            lambda x: '\n'.join(sorted(set(str(title) for title in x)))
        )
        columns = [
            content_column, 'Duplicate_Count', 'Unique_Pagetypes', 'Pagetype_List',
            'Titles', 'URLs'
        ]

    rollup = rollup[columns].sort_values('Duplicate_Count', ascending=False)

    # Create summary of duplicates
    duplicates = df.groupby(content_column).agg({
        'url': 'count',
        'pagetype': lambda x: ', '.join(set(x))
    }).reset_index()
    duplicates.columns = [content_column, 'Duplicate_Count', 'Pagetypes']
    duplicates = duplicates.sort_values('Duplicate_Count', ascending=False)

    # Create pagetype summary
    pagetype_summary = df.groupby('pagetype').agg({
        'url': 'count',
        content_column: lambda x: x.duplicated().sum()
    }).reset_index()
    pagetype_summary.columns = [
        'Pagetype', 'Total_URLs', f'Duplicate_{content_type}s'
    ]
    pagetype_summary['Duplication_Rate'] = (
        pagetype_summary[f'Duplicate_{content_type}s'] /
        pagetype_summary['Total_URLs'] * 100
    ).round(2)

    # Analyze duplicates by pagetype
    duplicate_analysis_by_pagetype = {}
    for pagetype in df['pagetype'].unique():
        pagetype_data = df[df['pagetype'] == pagetype]
        duplicates_in_pagetype = pagetype_data[
            pagetype_data.duplicated(subset=[content_column], keep=False)
        ]
        if not duplicates_in_pagetype.empty:
            duplicate_analysis_by_pagetype[pagetype] = duplicates_in_pagetype

    # N-gram analysis
    ngram_analyses = {}

    if include_ngrams and len(ngram_sizes) > 0:
        for n in ngram_sizes:
            ngram_counts_by_pagetype = {}

            for pagetype in df['pagetype'].unique():
                content = df[df['pagetype'] == pagetype][content_column]

                all_ngrams = []
                for text in content:
                    tokens = preprocess_text(text)
                    if tokens:
                        text_ngrams = extract_ngrams(tokens, n)
                        all_ngrams.extend(text_ngrams)

                ngram_counts = Counter(all_ngrams)
                frequent_ngrams = {
                    gram: count for gram, count in ngram_counts.items()
                    if count > 1
                }

                if frequent_ngrams:
                    ngram_counts_by_pagetype[pagetype] = frequent_ngrams

            if ngram_counts_by_pagetype:
                rows = []
                for pagetype, ngram_counts in ngram_counts_by_pagetype.items():
                    for ngram, count in ngram_counts.items():
                        rows.append({
                            'pagetype': pagetype,
                            'ngram': ngram,
                            'frequency': count
                        })

                if rows:
                    ngram_df = pd.DataFrame(rows)
                    ngram_df = ngram_df.sort_values(
                        ['pagetype', 'frequency'],
                        ascending=[True, False]
                    )
                    ngram_analyses[n] = ngram_df

    # Create detailed duplicate analysis
    detailed_rows = []
    for _, row in duplicates_df.iterrows():
        detailed_rows.append({
            'Content': row[content_column],
            'URL': row['url'],
            'Pagetype': row['pagetype'],
            'Other_Content': row['meta_description'] if content_column == 'title' else row['title']
        })

    detailed_df = pd.DataFrame(detailed_rows)

    return (
        duplicates,
        pagetype_summary,
        duplicate_analysis_by_pagetype,
        ngram_analyses,
        rollup,
        detailed_df
    )

def get_csv_download_link(df, filename):
    """Generate a download link for a dataframe."""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = (
        f'<a href="data:file/csv;base64,{b64}" '
        f'download="{filename}">Download {filename}</a>'
    )
    return href

def infer_column_mapping(columns):
    """Infer column mappings based on common patterns in column names."""
    patterns = {
        'url': ['url', 'link', 'address', 'location', 'full url', 'page url'],
        'title': ['title', 'page title', 'title tag', 'page title tag', 'seo title'],
        'meta_description': ['meta description', 'description', 'meta desc', 'metadescription', 'meta'],
        'pagetype': ['page type', 'pagetype', 'type', 'template', 'category', 'page category']
    }
    
    mapping = {}
    for col in columns:
        col_lower = col.lower()
        for field, pattern_list in patterns.items():
            if any(pattern in col_lower for pattern in pattern_list):
                mapping[field] = col
                break
    return mapping

def display_column_mapper(df):
    """Display column mapping interface and return the mapping."""
    st.subheader("Column Mapping")
    st.write("Please map your file columns to the required fields. The tool will try to automatically detect the correct mappings.")
    
    # Get columns from dataframe
    columns = list(df.columns)
    
    # Infer initial mapping
    inferred_mapping = infer_column_mapping(columns)
    
    # Create two columns for layout
    col1, col2 = st.columns([3, 2])
    
    with col1:
        st.markdown("### Map Your Columns")
        mapping = {}
        
        # Create mapping interface for each field
        required_fields = {
            'url': ('URL', True),
            'title': ('Title', True),
            'meta_description': ('Meta Description', True),
            'pagetype': ('Page Type', False)  # Optional
        }
        
        for field, (display_name, required) in required_fields.items():
            default_value = inferred_mapping.get(field, '')
            options = [''] + columns if not required else columns
            help_text = f"Select the column that contains your {display_name.lower()}"
            if not required:
                help_text += " (optional)"
                
            mapping[field] = st.selectbox(
                f"Select column for {display_name}",
                options=options,
                index=options.index(default_value) if default_value in options else 0,
                help=help_text
            )
    
    with col2:
        st.markdown("### Preview")
        if all(v for k, v in mapping.items() if required_fields[k][1]):
            preview_columns = [v for v in mapping.values() if v]
            preview_df = df[preview_columns].head(3)
            st.dataframe(preview_df, use_container_width=True)
            
            st.markdown("### Mapping Summary")
            for field, (display_name, required) in required_fields.items():
                if mapping[field]:
                    st.write(f"✓ {display_name}: `{mapping[field]}`")
                elif not required:
                    st.write("⚪ Page Type: Not mapped (optional)")
    
    # Confirm mapping button
    if st.button("Confirm Column Mapping", type="primary"):
        if all(v for k, v in mapping.items() if required_fields[k][1]):
            try:
                # Create new DataFrame with mapped columns
                mapped_df = pd.DataFrame()
                
                # Add required columns
                mapped_df['url'] = df[mapping['url']]
                mapped_df['title'] = df[mapping['title']]
                mapped_df['meta_description'] = df[mapping['meta_description']]
                
                # Add pagetype if mapped, otherwise use default
                if mapping['pagetype']:
                    mapped_df['pagetype'] = df[mapping['pagetype']]
                else:
                    mapped_df['pagetype'] = 'None'
                
                # Final check for required columns
                required_columns = {'url', 'title', 'meta_description', 'pagetype'}
                if not all(col in mapped_df.columns for col in required_columns):
                    st.error("Column mapping failed. Please check your column selections.")
                    return None
                
                st.success("Column mapping successful!")
                return mapped_df
                
            except Exception as e:
                st.error(f"Error mapping columns: {str(e)}")
                return None
        else:
            st.error("Please map all required columns before proceeding.")
            return None
            
    return None

def generate_summary_report(df, title_results, meta_results):
    """Generate a comprehensive summary report of the analysis."""
    # Verify required columns exist
    required_columns = {'Full URL', 'Title', 'Meta Description', 'pagetype'}
    if not all(col in df.columns for col in required_columns):
        return "Error: Missing required columns in the dataset."
        
    summary_text = f"""SEO Content Analysis Summary Report
=================================

Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

Overall Statistics
-----------------
Total URLs analyzed: {len(df):,}
Unique URLs: {df['Full URL'].nunique():,}
Total page types: {df['pagetype'].nunique():,}
"""

    # Only add title analysis if we have title results and the Title column exists
    if title_results and 'Title' in df.columns:
        summary_text += f"""
Title Analysis
-------------
Total titles: {len(df):,}
Unique titles: {df['Title'].nunique():,}
Duplicate titles: {len(df[df.duplicated(subset=['Title'], keep=False)]):,}
"""
        if title_results[4] is not None and not title_results[4].empty:
            summary_text += (
                f"Number of unique duplicate titles: {len(title_results[4]):,}\n"
                f"Most repeated title: {title_results[4].iloc[0]['Duplicate_Count']:,} occurrences\n"
            )

    # Only add meta description analysis if we have meta results and the Meta Description column exists
    if meta_results and 'Meta Description' in df.columns:
        summary_text += f"""
Meta Description Analysis
------------------------
Total meta descriptions: {len(df):,}
Unique meta descriptions: {df['Meta Description'].nunique():,}
Duplicate meta descriptions: {len(df[df.duplicated(subset=['Meta Description'], keep=False)]):,}
"""
        if meta_results[4] is not None and not meta_results[4].empty:
            summary_text += (
                f"Number of unique duplicate meta descriptions: {len(meta_results[4]):,}\n"
                f"Most repeated meta description: {meta_results[4].iloc[0]['Duplicate_Count']:,} occurrences\n"
            )

    summary_text += "\nAnalysis by Page Type\n--------------------\n"
    for pagetype in sorted(df['pagetype'].unique()):
        pagetype_data = df[df['pagetype'] == pagetype]
        summary_text += f"\nPage Type: {pagetype}\n"
        summary_text += f"Total URLs: {len(pagetype_data):,}\n"

        if title_results and 'Title' in df.columns:
            title_dupes = len(pagetype_data[
                pagetype_data.duplicated(subset=['Title'], keep=False)
            ])
            summary_text += f"Duplicate Titles: {title_dupes:,} "
            if title_dupes > 0:
                summary_text += f"({(title_dupes/len(pagetype_data)*100):.1f}%)\n"
            else:
                summary_text += "\n"

        if meta_results and 'Meta Description' in df.columns:
            meta_dupes = len(pagetype_data[
                pagetype_data.duplicated(subset=['Meta Description'], keep=False)
            ])
            summary_text += f"Duplicate Meta Descriptions: {meta_dupes:,} "
            if meta_dupes > 0:
                summary_text += f"({(meta_dupes/len(pagetype_data)*100):.1f}%)\n"
            else:
                summary_text += "\n"

    return summary_text

def create_zip_download(files_dict):
    """Create a zip file containing multiple analysis files.
    
    Args:
        files_dict (dict): Dictionary with filenames as keys and DataFrames/strings as values
    """
    zip_buffer = BytesIO()
    
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for filename, content in files_dict.items():
            if isinstance(content, pd.DataFrame):
                # Handle DataFrames
                csv_buffer = BytesIO()
                content.to_csv(csv_buffer, index=False)
                zip_file.writestr(filename, csv_buffer.getvalue())
            else:
                # Handle text content
                zip_file.writestr(filename, content.encode('utf-8'))
    
    return zip_buffer.getvalue()

def get_zip_download_link(zip_content, filename="analysis_results.zip"):
    """Generate a download link for a zip file."""
    b64 = base64.b64encode(zip_content).decode()
    href = (
        f'<a href="data:application/zip;base64,{b64}" '
        f'download="{filename}" class="download-button">'
        f'📥 Download All Results</a>'
    )
    return href

def display_summary_report(df, title_results, meta_results):
    """Display a visually appealing summary report using Streamlit components."""
    
    st.markdown("## 📊 Content Analysis Summary")
    st.markdown(f"*Analysis completed on {datetime.now().strftime('%B %d, %Y at %I:%M %p')}*")
    
    # Verify url column exists
    if 'url' not in df.columns:
        st.error("URL column not found in the dataset.")
        return
        
    # Overall Statistics
    st.markdown("### 📈 Overall Statistics")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total URLs", f"{len(df):,}")
    with col2:
        st.metric("Unique URLs", f"{df['url'].nunique():,}")
    with col3:
        st.metric("Page Types", f"{df['pagetype'].nunique():,}")
    
    # Title Analysis
    if title_results and 'title' in df.columns:
        st.markdown("### 📝 Title Analysis")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Titles", f"{len(df):,}")
        with col2:
            st.metric("Unique Titles", f"{df['title'].nunique():,}")
        with col3:
            dupes = len(df[df.duplicated(subset=['title'], keep=False)])
            st.metric("Duplicate Titles", f"{dupes:,}")
        
        if title_results[4] is not None and not title_results[4].empty:
            st.markdown("#### Most Duplicated Title")
            most_duped = title_results[4].iloc[0]
            st.info(
                f"**Title:** {most_duped['Title']}\n\n"
                f"**Times Used:** {most_duped['Duplicate_Count']:,}\n\n"
                f"**Page Types:** {most_duped['Pagetype_List']}"
            )
    
    # Meta Description Analysis
    if meta_results and 'meta_description' in df.columns:
        st.markdown("### 📄 Meta Description Analysis")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total Meta Descriptions", f"{len(df):,}")
        with col2:
            st.metric("Unique Meta Descriptions", f"{df['meta_description'].nunique():,}")
        with col3:
            dupes = len(df[df.duplicated(subset=['meta_description'], keep=False)])
            st.metric("Duplicate Meta Descriptions", f"{dupes:,}")
        
        if meta_results[4] is not None and not meta_results[4].empty:
            st.markdown("#### Most Duplicated Meta Description")
            most_duped = meta_results[4].iloc[0]
            st.info(
                f"**Meta Description:** {most_duped['Meta Description']}\n\n"
                f"**Times Used:** {most_duped['Duplicate_Count']:,}\n\n"
                f"**Page Types:** {most_duped['Pagetype_List']}"
            )
    
    # Page Type Analysis
    if 'pagetype' in df.columns:
        st.markdown("### 📑 Analysis by Page Type")
        
        for pagetype in sorted(df['pagetype'].unique()):
            pagetype_data = df[df['pagetype'] == pagetype]
            with st.expander(f"Page Type: {pagetype} ({len(pagetype_data):,} URLs)"):
                col1, col2 = st.columns(2)
                
                with col1:
                    if title_results and 'title' in df.columns:
                        title_dupes = len(pagetype_data[
                            pagetype_data.duplicated(subset=['title'], keep=False)
                        ])
                        dupe_rate = (title_dupes/len(pagetype_data)*100) if title_dupes > 0 else 0
                        st.metric(
                            "Duplicate Titles",
                            f"{title_dupes:,}",
                            f"{dupe_rate:.1f}% of pages"
                        )
                
                with col2:
                    if meta_results and 'meta_description' in df.columns:
                        meta_dupes = len(pagetype_data[
                            pagetype_data.duplicated(subset=['meta_description'], keep=False)
                        ])
                        dupe_rate = (meta_dupes/len(pagetype_data)*100) if meta_dupes > 0 else 0
                        st.metric(
                            "Duplicate Meta Descriptions",
                            f"{meta_dupes:,}",
                            f"{dupe_rate:.1f}% of pages"
                        )

def display_ngram_analysis(title_results, meta_results, analysis_type):
    """Display n-gram analysis results in a structured format."""
    
    if "Titles" in analysis_type and title_results is not None:
        st.subheader("Title N-grams")
        gram_tabs = st.tabs(["Bigrams", "Trigrams", "4-grams"])
        for i, gram_tab in enumerate(gram_tabs):
            with gram_tab:
                n = i + 2
                if title_results[3] is not None and n in title_results[3]:
                    st.dataframe(title_results[3][n])

    if "Meta Descriptions" in analysis_type and meta_results is not None:
        st.subheader("Meta Description N-grams")
        gram_tabs = st.tabs(["Bigrams", "Trigrams", "4-grams"])
        for i, gram_tab in enumerate(gram_tabs):
            with gram_tab:
                n = i + 2
                if meta_results[3] is not None and n in meta_results[3]:
                    st.dataframe(meta_results[3][n])

def display_export_options(df, title_results, meta_results, analysis_type):
    """Display and handle export options for analysis results."""
    
    st.subheader("Download Analysis Results")
    
    # Create dictionary of all files to be included in zip
    export_files = {}
    
    if "Titles" in analysis_type and title_results[4] is not None:
        export_files['title_duplicate_rollup.csv'] = title_results[4]
        export_files['title_pagetype_summary.csv'] = title_results[1]
        
        # Add n-gram analyses if they exist
        if title_results[3]:
            for n, df_ngram in title_results[3].items():
                export_files[f'title_{n}gram_analysis.csv'] = df_ngram

    if "Meta Descriptions" in analysis_type and meta_results is not None:
        if meta_results[4] is not None:
            export_files['meta_description_duplicate_rollup.csv'] = meta_results[4]
        export_files['meta_description_pagetype_summary.csv'] = meta_results[1]
        
        # Add n-gram analyses if they exist
        if meta_results[3]:
            for n, df_ngram in meta_results[3].items():
                export_files[f'meta_description_{n}gram_analysis.csv'] = df_ngram
    
    # Generate summary report
    summary_text = generate_summary_report(df, title_results, meta_results)
    export_files['analysis_summary.txt'] = summary_text
    
    # Create and display the zip download button
    zip_content = create_zip_download(export_files)
    st.markdown(
        get_zip_download_link(zip_content),
        unsafe_allow_html=True
    )
    
    st.markdown("---")
    
    # Display visual summary report
    display_summary_report(df, title_results, meta_results)
    
    st.markdown("---")
    st.markdown("### Individual File Downloads")
    
    # Display individual file download links
    for filename, content in export_files.items():
        if isinstance(content, pd.DataFrame):
            st.markdown(
                get_csv_download_link(content, filename),
                unsafe_allow_html=True
            )
        else:
            b64 = base64.b64encode(content.encode()).decode()
            st.markdown(
                f'<a href="data:text/plain;base64,{b64}" '
                f'download="{filename}">Download {filename}</a>',
                unsafe_allow_html=True
            )

def main():
    st.title("SEO Content Analysis Tool")
    st.write("""
    This tool analyzes SEO content (titles and meta descriptions) across different page types,
    identifying patterns, duplicates, and n-gram frequencies.
    """)

    # File upload
    uploaded_file = st.file_uploader(
        "Upload your CSV/TSV file",
        type=['csv', 'tsv'],
        help="File should contain columns for URL, Title, Meta Description, and Page Type"
    )

    if uploaded_file:
        try:
            # File configuration in sidebar
            st.sidebar.header("File Settings")
            delimiter = st.sidebar.selectbox(
                "File delimiter",
                options=[',', '\t', ';', '|'],
                index=0,
                format_func=lambda x: 'Tab' if x == '\t' else x
            )

            # Read the file
            df = pd.read_csv(uploaded_file, sep=delimiter)
            if not st.session_state.mapping_complete:
                mapped_df = display_column_mapper(df)
                if mapped_df is not None:
                    st.session_state.mapped_df = mapped_df
                    st.session_state.mapping_complete = True
                    st.success("Column mapping confirmed! You can now proceed with the analysis.")
                    st.session_state.analysis_complete = False
                    st.session_state.results = {}
                    # Remove the return statement here to allow the code to continue
            
            # Only proceed with analysis if mapping is complete
            if st.session_state.mapping_complete and st.session_state.mapped_df is not None:
                df = st.session_state.mapped_df  # Use the mapped DataFrame

                # Verify required columns exist
                required_columns = {'url', 'title', 'meta_description', 'pagetype'}
                if not all(col in df.columns for col in required_columns):
                    st.error("Missing required columns. Please check your column mapping.")
                    return

                # Display basic file info
                st.subheader("File Overview")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total URLs", f"{len(df):,}")
                with col2:
                    st.metric("Unique URLs", f"{df['url'].nunique():,}")
                with col3:
                    st.metric("Page Types", f"{df['pagetype'].nunique():,}")

                # Analysis configuration
                st.sidebar.subheader("Analysis Options")
                analysis_type = st.sidebar.multiselect(
                    "Select content to analyze",
                    options=["Titles", "Meta Descriptions"],
                    default=["Titles", "Meta Descriptions"],
                    help="Choose which content types to analyze"
                )

                include_ngrams = st.sidebar.checkbox(
                    "Include N-gram Analysis",
                    value=True,
                    help="Enable to analyze word patterns (may increase processing time)"
                )

                ngram_sizes = []
                if include_ngrams:
                    ngram_sizes = st.sidebar.multiselect(
                        "Select N-gram sizes",
                        options=["Bigrams (2)", "Trigrams (3)", "4-grams (4)"],
                        default=["Bigrams (2)", "Trigrams (3)", "4-grams (4)"],
                        help="Choose which n-gram sizes to analyze"
                    )

                # Add custom styling for the button
                st.markdown("""
                    <style>
                    div.stButton > button:first-child {
                        background-color: #FF4B4B;
                        color: white;
                        height: 3em;
                        width: 100%;
                        font-size: 20px;
                        font-weight: bold;
                        border: none;
                        border-radius: 4px;
                        margin: 1em 0;
                    }
                    div.stButton > button:hover {
                        background-color: #FF6B6B;
                    }
                    </style>
                """, unsafe_allow_html=True)

                # Create a centered container for the button
                col1, col2, col3 = st.columns([1, 2, 1])
                with col2:
                    if st.button("🔍 Run Analysis"):
                        with st.spinner("Analyzing content..."):
                            title_results = None
                            meta_results = None

                            # Extract numbers from n-gram selections
                            selected_sizes = []
                            if include_ngrams and ngram_sizes:
                                for size_option in ngram_sizes:
                                    number = re.search(r'\((\d+)\)', size_option)
                                    if number:
                                        selected_sizes.append(int(number.group(1)))

                            # Title analysis
                            if "Titles" in analysis_type:
                                st.info("Analyzing titles...")
                                title_results = analyze_content(
                                    df,
                                    'Title',
                                    include_ngrams=include_ngrams,
                                    ngram_sizes=selected_sizes
                                )

                            # Meta description analysis
                            if "Meta Descriptions" in analysis_type:
                                st.info("Analyzing meta descriptions...")
                                meta_results = analyze_content(
                                    df,
                                    'Meta Description',
                                    include_ngrams=include_ngrams,
                                    ngram_sizes=selected_sizes
                                )

                            # Store results in session state
                            st.session_state.results = {
                                'title': title_results,
                                'meta': meta_results
                            }
                            st.session_state.analysis_complete = True
                            st.success("Analysis complete!")

                # Display results if analysis is complete
                if st.session_state.analysis_complete:
                    title_results = st.session_state.results['title']
                    meta_results = st.session_state.results['meta']

                    st.header("Analysis Results")

                    # Create dynamic tabs based on selected analyses
                    tab_options = []
                    if "Titles" in analysis_type:
                        tab_options.append("Title Analysis")
                    if "Meta Descriptions" in analysis_type:
                        tab_options.append("Meta Description Analysis")
                    if include_ngrams and len(ngram_sizes) > 0:
                        tab_options.append("N-gram Analysis")
                    tab_options.append("Export Results")
                    
                    if tab_options:  # Only create tabs if there are options
                        tabs = st.tabs(tab_options)
                        tab_index = 0

                        # Title Analysis Tab
                        if "Titles" in analysis_type:
                            with tabs[tab_index]:
                                st.subheader("Title Duplicate Analysis")
                                if title_results and title_results[4] is not None:
                                    st.dataframe(title_results[4])
                                    st.markdown(
                                        get_csv_download_link(
                                            title_results[4],
                                            "title_duplicate_rollup.csv"
                                        ),
                                        unsafe_allow_html=True
                                    )
                                st.subheader("Title Duplication by Page Type")
                                if title_results:
                                    st.dataframe(title_results[1])
                            tab_index += 1

                        # Meta Description Analysis Tab
                        if "Meta Descriptions" in analysis_type:
                            with tabs[tab_index]:
                                st.subheader("Meta Description Duplicate Analysis")
                                if meta_results and meta_results[4] is not None:
                                    st.dataframe(meta_results[4])
                                    st.markdown(
                                        get_csv_download_link(
                                            meta_results[4],
                                            "meta_description_duplicate_rollup.csv"
                                        ),
                                        unsafe_allow_html=True
                                    )
                                st.subheader("Meta Description Duplication by Page Type")
                                if meta_results:
                                    st.dataframe(meta_results[1])
                            tab_index += 1

                        # N-gram Analysis Tab
                        if include_ngrams and len(ngram_sizes) > 0:
                            with tabs[tab_index]:
                                display_ngram_analysis(title_results, meta_results, analysis_type)
                            tab_index += 1

                        # Export Results Tab
                        with tabs[tab_index]:
                            display_export_options(df, title_results, meta_results, analysis_type)

        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            with st.expander("Show Debug Information"):
                st.code(traceback.format_exc())
    
    else:
        st.info("Please upload a CSV or TSV file to begin.")

if __name__ == "__main__":
    main()

================
File: notes.md
================
- add an input to create pagetypes if they don't already have input

make sure it cleans the urls and adds a unique identifier based on site section? look for code that parses the urls and adds it.

create multiple alternates and allow prompt for matching at the streamlit level with headers

add a download all button

================
File: README.md
================
# SEO Content Analysis Tool

## Overview
A Streamlit-based web application for analyzing SEO content across different page types. This tool helps identify patterns, duplicates, and n-gram frequencies in titles and meta descriptions, making it easier to maintain content consistency and identify optimization opportunities.

## Features
- Interactive web interface built with Streamlit
- Support for CSV and TSV file formats
- Comprehensive duplicate content analysis
- N-gram analysis (bigrams, trigrams, 4-grams)
- Page type breakdown and statistics
- Downloadable analysis reports
- Real-time progress tracking
- Interactive data visualization
- Configurable processing parameters

## Requirements
- Python 3.8+
- Required packages:
  ```
  streamlit>=1.28.0
  pandas>=1.5.0
  nltk>=3.8.1
  tqdm>=4.65.0
  ```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/seo-content-analysis.git
cd seo-content-analysis
```

2. Create and activate a virtual environment (recommended):
```bash
# On macOS/Linux
python -m venv venv
source venv/bin/activate

# On Windows
python -m venv venv
venv\Scripts\activate
```

3. Install required packages:
```bash
pip install -r requirements.txt
```

## Usage

1. Start the Streamlit app:
```bash
streamlit run app.py
```

2. Open your web browser and navigate to the provided URL (typically http://localhost:8501)

3. Upload your CSV/TSV file containing the following required columns:
   - Full URL
   - pagetype
   - Title
   - Meta Description

4. Configure analysis settings in the sidebar:
   - Select the appropriate file delimiter
   - Adjust chunk size if needed

5. Click "Run Analysis" to start processing

## Input File Requirements

Your input file should be a CSV or TSV with the following columns:
- `Full URL`: The complete URL of the page
- `pagetype`: The category or type of the page
- `Title`: The page's title tag content
- `Meta Description`: The page's meta description content

Example format:
```csv
Full URL,pagetype,Title,Meta Description
https://example.com/page1,blog,"First Blog Post","This is a description of the first blog post"
https://example.com/page2,product,"Product Name","Product description here"
```

## Analysis Features

### Title Analysis
- Identifies duplicate titles across all pages
- Groups duplicates by page type
- Calculates duplication rates
- Provides detailed duplicate instances

### Meta Description Analysis
- Finds duplicate meta descriptions
- Groups by page type
- Shows duplication patterns
- Lists affected URLs

### N-gram Analysis
- Analyzes frequent word patterns
- Supports 2-gram, 3-gram, and 4-gram analysis
- Breaks down patterns by page type
- Shows frequency counts

### Export Options
- CSV exports for all analyses
- Comprehensive summary report
- Detailed breakdown by page type
- N-gram frequency reports

## Output Files

The tool generates several downloadable files:
1. Duplicate Analysis:
   - `title_duplicate_rollup.csv`
   - `meta_description_duplicate_rollup.csv`
   - `title_pagetype_summary.csv`
   - `meta_description_pagetype_summary.csv`

2. N-gram Analysis:
   - `title_2gram_analysis.csv`
   - `title_3gram_analysis.csv`
   - `title_4gram_analysis.csv`
   - `meta_description_2gram_analysis.csv`
   - `meta_description_3gram_analysis.csv`
   - `meta_description_4gram_analysis.csv`

3. Summary:
   - `analysis_summary.txt`

## Performance Tips

- For large files (>1GB), increase the chunk size in the sidebar
- Close other memory-intensive applications when processing large datasets
- Use a machine with at least 8GB RAM for optimal performance

## Error Handling

The tool includes comprehensive error handling for:
- Missing or invalid columns
- File format issues
- Memory limitations
- Processing errors

If you encounter errors:
1. Check your input file format
2. Verify required columns are present
3. Adjust chunk size if needed
4. Ensure sufficient system resources

## Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

1. Fork the repository
2. Create your feature branch
3. Make your changes
4. Submit a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For bugs, questions, and discussions, please use the GitHub Issues page.

## Acknowledgments

- Built with Streamlit
- Uses NLTK for natural language processing
- Powered by pandas for data analysis

================
File: requirements.txt
================
streamlit>=1.28.0
pandas>=1.5.0
nltk>=3.8.1
tqdm>=4.65.0
